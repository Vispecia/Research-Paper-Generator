 deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name âconvolut neural networkâ indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ãpixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ãã wide high color channel singl fulli connect neuron first hidden layer regular neural network weight Ã imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w â k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k â textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size Ã appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ânorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh â¡ x displaystyl fxtanhx f x tanh â¡ x displaystyl fxtanhx sigmoid function Ï x e â x â textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label â â â displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu Ã larg input volum may warrant Ã pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl â p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl â p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w â displaystyl vec w everi neuron satisfi â w â â c displaystyl vec wc typic valu c displaystyl c order â paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep deep deep deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name convolut neural network indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size  wide high color channel singl fulli connect neuron first hidden layer regular neural network weight  imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name convolut neural network indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size  wide high color channel singl fulli connect neuron first hidden layer regular neural network weight  imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name convolut neural network indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size  wide high color channel singl fulli connect neuron first hidden layer regular neural network weight  imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn deep learn convolut neural network cnn convnet class deep neural network commonli appli analyz visual imageri also known shift invari space invari artifici neural network siann base sharedweight architectur translat invari characterist applic imag video recognit recommend system imag classif medic imag analysi natur languag process financi time seri cnn regular version multilay perceptron multilay perceptron usual mean fulli connect network neuron one layer connect neuron next layer fullyconnected network make prone overfit data typic way regular includ ad form magnitud measur weight loss function cnn take differ approach toward regular take advantag hierarch pattern data assembl complex pattern use smaller simpler pattern therefor scale connected complex cnn lower extrem convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl preprocess compar imag classif algorithm mean network learn filter tradit algorithm handengin independ prior knowledg human effort featur design major advantag name “convolut neural network” indic network employ mathemat oper call convolut convolut special kind linear oper convolut network simpli neural network use convolut place gener matrix multipl least one layer convolut neural network consist input output layer well multipl hidden layer hidden layer cnn typic consist seri convolut layer convolv multipl dot product activ function commonli relu layer subsequ follow addit convolut pool layer fulli connect layer normal layer refer hidden layer input output mask activ function final convolut though layer colloqui refer convolut convent mathemat technic slide dot product crosscorrel signific indic matrix affect weight determin specif index point program cnn input tensor shape number imag x imag width x imag height x imag depth pass convolut layer imag becom abstract featur map shape number imag x featur map width x featur map height x featur map channel convolut layer within neural network follow attribut convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network can use learn featur well classifi data practic appli architectur imag high number neuron necessari even shallow opposit deep architectur due larg input size associ imag pixel relev variabl instanc fulli connect layer small imag size x weight neuron second layer convolut oper bring solut problem reduc number free paramet allow network deeper fewer paramet instanc regardless imag size tile region size x share weight requir learnabl paramet way resolv vanish explod gradient problem train tradit multilay neural network mani layer use backpropagationcit need convolut network may includ local global pool layer streamlin underli comput pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster typic x global pool act neuron convolut layer addit pool may comput max averag max pool use maximum valu cluster neuron prior layer averag pool use averag valu cluster neuron prior layer fulli connect layer connect everi neuron one layer everi neuron anoth layer principl tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer fulli connect layer neuron receiv input everi element previou layer convolut layer neuron receiv input restrict subarea previou layer typic subarea squar shape eg size input area neuron call recept field fulli connect layer recept field entir previou layer convolut layer recept area smaller entir previou layer neuron neural network comput output valu appli specif function input valu come recept field previou layer function appli input valu determin vector weight bia typic real number learn neural network progress make iter adjust bias weight vector weight bia call filter repres particular featur input eg particular shape distinguish featur cnn mani neuron can share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight cnn design follow vision process live organismscit need work hubel wiesel s s show cat monkey visual cortex contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept fieldcit need neighbor cell similar overlap recept fieldscit need recept field size locat vari systemat across cortex form complet map visual spacecit need cortex hemispher repres contralater visual fieldcit need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task neocognitron introduc kunihiko fukushima inspir abovement work hubel wiesel neocognitron introduc two basic type layer cnn convolut layer downsampl layer convolut layer contain unit whose recept field cover patch previou layer weight vector set adapt paramet unit often call filter unit can share filter downsampl layer contain unit whose recept field cover patch previou convolut layer unit typic comput averag activ unit patch downsampl help correctli classifi object visual scene even object shift variant neocognitron call cresceptron instead use fukushima spatial averag j weng et al introduc method call maxpool downsampl unit comput maximum activ unit patch maxpool often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag neocognitron first cnn requir unit locat multipl network posit share weight neocognitron adapt analyz timevari signal time delay neural network tdnn introduc alex waibel et al first convolut network achiev shift invari util weight share combin backpropag train thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process timeinvariantli hampshir waibel introduc variant perform two dimension convolut sinc tdnn oper spectrogram result phonem recognit system invari shift time frequenc inspir translat invari imag process cnn tile neuron output can cover time stage tdnn now achiev best perform far distanc speech recognit yamaguchi et al introduc concept max pool combin tdnn max pool order realiz speaker independ isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif system recogn handwritten zip code number involv convolut kernel coeffici labori hand design yann lecun et al use backpropag learn convolut kernel coeffici directli imag handwritten number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type approach becam foundat modern comput vision lenet pioneer level convolut network lecun et al classifi digit appli sever bank recogn handwritten number check british english chequ digit x pixel imag abil process higher resolut imag requir larger layer convolut neural network techniqu constrain avail comput resourc similarli shift invari neural network propos w zhang et al imag charact recognit architectur train algorithm modifi appli medic imag process automat detect breast cancer mammogram differ convolutionbas design propos applic decomposit onedimension electromyographi convolv signal via deconvolut design modifi deconvolutionbas design feedforward architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model imagelik output highest resolut gener eg semant segment imag reconstruct object local task although cnn invent s breakthrough s requir fast implement graphic process unit gpu shown k s oh k jung standard neural network can greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first gpuimplement cnn describ k chellapilla et al implement time faster equival implement cpu subsequ work also use gpu initi type neural network differ cnn especi unsupervis neural network dan ciresan et al idsia show even deep standard neural network mani layer can quickli train gpu supervis learn old method known backpropag network outperform previou machin learn method mnist handwritten digit benchmark extend gpu approach cnn achiev acceler factor impress result use cnn gpu win imag recognit contest achiev superhuman perform first time may septemb cnn won less four imag competit also significantli improv best perform literatur multipl imag databas includ mnist databas norb databas hwdb dataset chines charact cifar dataset dataset x label rgb imag subsequ similar gpubas cnn alex krizhevski et al won imagenet larg scale visual recognit challeng deep cnn layer microsoft won imagenet contest compar train cnn use gpu much attent given intel xeon phi coprocessor notabl develop parallel method train convolut neural network intel xeon phi name control hogwild arbitrari order synchron chao chao exploit thread simdlevel parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognitionexampl need howev due full connect node suffer curs dimension scale well higher resolut imag ×pixel imag rgb color channel million weight high feasibl process effici scale full connect exampl cifar imag size ×× wide high color channel singl fulli connect neuron first hidden layer regular neural network weight × imag howev lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer imag data comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network biolog inspir variant multilay perceptron design emul behavior visual cortexcit need model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum eg hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product entri filter input produc dimension activ map filter result network learn filter activ detect specif type featur spatial posit inputnb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum can thu also interpret output neuron look small region input share paramet neuron activ map deal highdimension input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride zeropad spatial size output volum can comput function input volum size w displaystyl w kernel field size convolut layer neuron k displaystyl k stride appli s displaystyl s amount zero pad p displaystyl p use border formula calcul mani neuron fit given volum given w − k p s displaystyl frac wkp number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k − textstyl pk stride s displaystyl s ensur input volum output volum will size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use just portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl dimension slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer can comput convolut neuron weight input volumenb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ eyespecif hairspecif featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool form nonlinear downsampl sever nonlinear function implement pool among max pool common partit input imag set nonoverlap rectangl subregion output maximum intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit common period insert pool layer success convolut layer cnn architecturecit need pool oper provid anoth form translat invari pool layer oper independ everi depth slice input resiz spatial common form pool layer filter size × appli stride downsampl everi depth slice input along width height discard activ f x y s max b s x y b displaystyl fxysmax absxayb case everi max oper number depth dimens remain unchang addit max pool pool unit can use function averag pool ℓnorm pool averag pool often use histor recent fallen favor compar max pool perform better practic due aggress reduct size representationwhich recent trend toward use smaller filter discard pool layer altogeth region interest pool also known roi pool variant max pool output size fix input rectangl paramet pool import compon convolut neural network object detect base fast rcnn architectur relu abbrevi rectifi linear unit appli nonsatur activ function f x max x textstyl fxmaxx effect remov neg valu activ map set zero increas nonlinear properti decis function overal network without affect recept field convolut layer function also use increas nonlinear exampl satur hyperbol tangent f x tanh ⁡ x displaystyl fxtanhx f x tanh ⁡ x displaystyl fxtanhx sigmoid function σ x e − x − textstyl sigma xex relu often prefer function train neural network sever time faster without signific penalti gener accuraci final sever convolut max pool layer highlevel reason neural network done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular nonconvolut artifici neural network activ can thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer specifi train penal deviat predict output true label normal final layer neural network variou loss function appropri differ task may use softmax loss use predict singl class k mutual exclus classesnb sigmoid crossentropi loss use predict k independ probabl valu displaystyl euclidean loss use regress realvalu label − ∞ ∞ displaystyl infti infti cnn use hyperparamet standard multilay perceptron mlp usual rule learn rate regular constant still appli follow kept mind optim sinc featur map size decreas depth layer near input layer will tend fewer filter higher layer can equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input requir keep total number activ number featur map time number pixel posit nondecreas one layer next number featur map directli control capac depend number avail exampl task complex common filter shape found literatur vari greatli usual chosen base dataset challeng thu find right level granular creat abstract proper scale given particular dataset without overfit typic valu × larg input volum may warrant × pool lower layer howev choos larger shape will dramat reduc dimens signal may result excess inform loss often nonoverlap pool window perform best regular process introduc addit inform solv illpos problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout train stage individu node either drop net probabl − p displaystyl p kept probabl p displaystyl p reduc network left incom outgo edg droppedout node also remov reduc network train data stage remov node reinsert network origin weight train stage probabl hidden node will drop usual input node much lower intuit inform directli lost input node ignor test time train finish ideal like find sampl averag possibl n displaystyl n droppedout network unfortun unfeas larg valu n displaystyl n howev can find approxim use full network node output weight factor p displaystyl p expect valu output node train stage biggest contribut dropout method although effect gener n displaystyl n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featuresclarif need better gener new data dropconnect gener dropout connect rather output unit can drop probabl − p displaystyl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet can combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ sinc degre model overfit determin power amount train receiv provid convolut network train exampl can reduc overfit sinc network usual train avail data one approach either gener new data scratch possibl perturb exist data creat new one exampl input imag asymmetr crop percent creat new exampl label origin one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function can perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight l norm squar magnitud l norm weight vector error node level accept model complex can reduc increas proportion constant thu increas penalti larg weight vector l regular common form regular can implement penal squar magnitud paramet directli object l regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot l regular anoth common form possibl combin l l regular call elast net regular l regular lead weight vector becom spars optim word neuron l regular end use spars subset import input becom nearli invari noisi input anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w → displaystyl vec w everi neuron satisfi ‖ w → ‖ c displaystyl vec wc typic valu c displaystyl c order – paper report improv use form regular pool lose precis spatial relationship highlevel part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape can recogn differ viewpoint current common way deal problem train network transform data differ orient scale light etc network can cope variat comput intens larg dataset altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within done larg featur can recogn use consist pose part eg nose mouth pose make consist predict pose whole face use approach ensur higher level entiti eg face present lower level eg nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate percent mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet won imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report percent recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc largescal visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object finegrain categori particular breed dog speci bird wherea convolut neural network handl thiscit need manylay cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long shortterm memori lstm recurr unit typic incorpor cnn account interfram interclip depend unsupervis learn scheme train spatiotempor featur introduc base convolut gate restrict boltzmann machin independ subspac analysi cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task cnn d convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein can identifi potenti treatment atomwis introduc atomnet first deep learn neural network structurebas ration drug design system train directli dimension represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat sp carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn can natur tailor analyz suffici larg collect time seri data repres oneweeklong human physic activ stream augment rich clinic data includ death regist provid eg nhane studi simpl cnn combin coxgompertz proport hazard model use produc proofofconcept exampl digit biomark age form allcausesmort predictor cnn use game checker fogel chellapilla publish paper show convolut neural network learn play checker use coevolut learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program blondi test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg layer convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network can perform compar even better dilat convolut might enabl onedimension convolut neural network effect learn time seri depend convolut can implement effici rnnbase solut suffer vanish explod gradient convolut network can provid improv forecast perform multipl similar time seri learn cnn can also appli task time seri analysi eg time seri classif quantil forecast mani applic train data less avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use indomain data finetun network weight allow convolut network success appli problem small train set endtoend train predict common practic comput vision howev human interpret explan requir critic system selfdriv car recent advanc visual salienc spatial tempor attent critic spatial regionstempor instant visual justifi cnn predict deep qnetwork dqn type deep learn model combin deep neural network qlearn form reinforc learn unlik earlier reinforc learn agent dqn util cnn can learn directli highdimension sensori inputscit need preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit d structur imag like cnn make use pretrain like deep belief network provid gener structur can use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn